{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 허깅페이스 임베딩(HuggingFace Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH08-Embeddings\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH08-Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ./cache/ 경로에 다운로드 받도록 설정\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 샘플 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"안녕, 만나서 반가워.\",\n",
    "    \"LangChain simplifies the process of building applications with large language models\",\n",
    "    \"랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. \",\n",
    "    \"LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**참고(Reference)**\n",
    "\n",
    "![](./images/top-ranked-embeddings.png)\n",
    "\n",
    "- [(출처) Kor-IR: 한국어 검색을 위한 임베딩 벤치마크](https://github.com/teddylee777/Kor-IR?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Endpoint Embedding\n",
    "\n",
    "`HuggingFaceEndpointEmbeddings` 는 내부적으로 InferenceClient를 사용하여 임베딩을 계산한다는 점에서 HuggingFaceEndpoint가 LLM에서 수행하는 것과 매우 유사합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "model_name = \"Qwen/Qwen3-Embedding-8B\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=model_name,\n",
    "    task=\"feature-extraction\",\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document 임베딩은 `embed_documents()` 를 호출하여 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 219 ms\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/pipeline/feature-extraction/Qwen/Qwen3-Embedding-8B (Request ID: Root=1-68a4f138-70e94064207468fd21907d92;3e45441b-da50-4f0d-94f7-3a0492b92865)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\younghl\\CursorProjects\\langchain-kr\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\younghl\\CursorProjects\\langchain-kr\\.venv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/pipeline/feature-extraction/Qwen/Qwen3-Embedding-8B",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m# Document Embedding 수행\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43membedded_documents = hf_embeddings.embed_documents(texts)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\younghl\\CursorProjects\\langchain-kr\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\younghl\\CursorProjects\\langchain-kr\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1470\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1469\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1471\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\younghl\\CursorProjects\\langchain-kr\\.venv\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:1434\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1432\u001b[39m st = clock2()\n\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1436\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\younghl\\CursorProjects\\langchain-kr\\.venv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface_endpoint.py:112\u001b[39m, in \u001b[36mHuggingFaceEndpointEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    110\u001b[39m _model_kwargs = \u001b[38;5;28mself\u001b[39m.model_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m#  api doc: https://huggingface.github.io/text-embeddings-inference/#/Text%20Embeddings%20Inference/embed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m responses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minputs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m json.loads(responses.decode())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\younghl\\CursorProjects\\langchain-kr\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:132\u001b[39m, in \u001b[36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m     warning_message += \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + message\n\u001b[32m    131\u001b[39m warnings.warn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\younghl\\CursorProjects\\langchain-kr\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:268\u001b[39m, in \u001b[36mInferenceClient.post\u001b[39m\u001b[34m(self, json, data, model, task, stream)\u001b[39m\n\u001b[32m    266\u001b[39m url = provider_helper._prepare_url(\u001b[38;5;28mself\u001b[39m.token, mapped_model)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    267\u001b[39m headers = provider_helper._prepare_headers(\u001b[38;5;28mself\u001b[39m.headers, \u001b[38;5;28mself\u001b[39m.token)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRequestParameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munknown\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\younghl\\CursorProjects\\langchain-kr\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:321\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    318\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\younghl\\CursorProjects\\langchain-kr\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:481\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://router.huggingface.co/hf-inference/pipeline/feature-extraction/Qwen/Qwen3-Embedding-8B (Request ID: Root=1-68a4f138-70e94064207468fd21907d92;3e45441b-da50-4f0d-94f7-3a0492b92865)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Document Embedding 수행\n",
    "embedded_documents = hf_embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[HuggingFace Endpoint Embedding]\")\n",
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Embedding 수행\n",
    "embedded_query = hf_embeddings.embed_query(\"LangChain 에 대해서 알려주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유사도 계산\n",
    "\n",
    "**벡터 내적을 통한 유사도 계산**\n",
    "- 벡터 내적(dot product)을 사용하여 유사도를 계산합니다. \n",
    "\n",
    "- 유사도 계산 공식:\n",
    "\n",
    "$$ \\text{similarities} = \\mathbf{query} \\cdot \\mathbf{documents}^T $$\n",
    "\n",
    "#### 벡터 내적의 수학적 의미\n",
    "\n",
    "**벡터 내적 정의**\n",
    "\n",
    "벡터 $\\mathbf{a}$와 $\\mathbf{b}$의 내적은 다음과 같이 정의됩니다:\n",
    "$$ \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i $$\n",
    "\n",
    "**코사인 유사도와의 관계**\n",
    "\n",
    "벡터 내적은 다음과 같은 성질을 가집니다.\n",
    "$$ \\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos \\theta $$\n",
    "\n",
    "여기서,\n",
    "- $\\|\\mathbf{a}\\|$와 $\\|\\mathbf{b}\\|$는 각각 벡터 $\\mathbf{a}$와 $\\mathbf{b}$의 크기(노름, Euclidean norm)입니다.\n",
    "- $\\theta$는 두 벡터 사이의 각도입니다.\n",
    "- $\\cos \\theta$는 두 벡터 사이의 코사인 유사도입니다.\n",
    "\n",
    "**벡터 내적의 유사도 해석**\n",
    "내적 값이 클수록 (양의 큰 값일수록),\n",
    "- 두 벡터의 크기($\\|\\mathbf{a}\\|$와 $\\|\\mathbf{b}\\|$)가 크고,\n",
    "- 두 벡터 사이의 각도($\\theta$)가 작으며 ($\\cos \\theta$가 1에 가까움),\n",
    "\n",
    "이는 두 벡터가 유사한 방향을 가리키고, 크기가 클수록 더 유사하다는 것을 의미합니다.\n",
    "\n",
    "**벡터의 크기(노름) 계산**\n",
    "\n",
    "Euclidean norm 정의\n",
    "벡터 $\\mathbf{a} = [a_1, a_2, \\ldots, a_n]$에 대해, Euclidean norm $\\|\\mathbf{a}\\|$는 다음과 같이 정의됩니다:\n",
    "$$ \\|\\mathbf{a}\\| = \\sqrt{a_1^2 + a_2^2 + \\cdots + a_n^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "query 와 embedding_document 간의 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedded_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 질문(embedded_query): LangChain 에 대해서 알려주세요.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m np.array(\u001b[43membedded_query\u001b[49m) @ np.array(embedded_documents).T\n",
      "\u001b[31mNameError\u001b[39m: name 'embedded_query' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 질문(embedded_query): LangChain 에 대해서 알려주세요.\n",
    "np.array(embedded_query) @ np.array(embedded_documents).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = (np.array(embedded_query) @ np.array(embedded_documents).T).argsort()[::-1]\n",
    "sorted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Query] LangChain 에 대해서 알려주세요.\\n====================================\")\n",
    "for i, idx in enumerate(sorted_idx):\n",
    "    print(f\"[{i}] {texts[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `intfloat/multilingual-e5-large-instruct`\n",
    "\n",
    "- [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)\n",
    "- [intfloat/multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "# model_name = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cpu\"},  # cuda, cpu\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Document\n",
    "embedded_documents1 = hf_embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tintfloat/multilingual-e5-large-instruct\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embedded_documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDimension: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43membedded_documents\u001b[49m[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'embedded_documents' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## BGE-M3 임베딩\n",
    "\n",
    "아래의 옵션 중 에러가 발생할 수 있는 옵션 설명\n",
    "\n",
    "- `{\"device\": \"mps\"}`: GPU 대신 MPS를 사용하여 임베딩 계산을 수행합니다. (Mac 사용자)\n",
    "- `{\"device\": \"cuda\"}`: GPU를 사용하여 임베딩 계산을 수행합니다. (Linux, Windows 사용자, 단 CUDA 설치 필요)\n",
    "- `{\"device\": \"cpu\"}`: CPU를 사용하여 임베딩 계산을 수행합니다. (모든 사용자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Document\n",
    "embedded_documents = hf_embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tBAAI/bge-m3\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query] LangChain 에 대해서 알려주세요.\n",
      "====================================\n",
      "[0] LangChain simplifies the process of building applications with large language models\n",
      "\n",
      "[1] LangChain은 초거대 언어모델로 애플리케이션을 구축하는 과정을 단순화합니다.\n",
      "\n",
      "[2] 랭체인 한국어 튜토리얼은 LangChain의 공식 문서, cookbook 및 다양한 실용 예제를 바탕으로 하여 사용자가 LangChain을 더 쉽고 효과적으로 활용할 수 있도록 구성되어 있습니다. \n",
      "\n",
      "[3] 안녕, 만나서 반가워.\n",
      "\n",
      "[4] Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedded_query = hf_embeddings.embed_query(\"LangChain 에 대해서 알려주세요.\")\n",
    "embedded_documents = hf_embeddings.embed_documents(texts)\n",
    "\n",
    "# 질문(embedded_query): LangChain 에 대해서 알려주세요.\n",
    "np.array(embedded_query) @ np.array(embedded_documents).T\n",
    "\n",
    "sorted_idx = (np.array(embedded_query) @ np.array(embedded_documents).T).argsort()[::-1]\n",
    "\n",
    "print(\"[Query] LangChain 에 대해서 알려주세요.\\n====================================\")\n",
    "for i, idx in enumerate(sorted_idx):\n",
    "    print(f\"[{i}] {texts[idx]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `FlagEmbedding` 을 활용하는 방식\n",
    "\n",
    "**참고**\n",
    "- [FlagEmbedding - BGE-M3 Usage](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#usage)\n",
    "\n",
    "\n",
    "`FlagEmbedding` 에서 제공하는 세 가지 접근법을 조합하면, 더욱 강력한 검색 시스템을 구축할 수 있습니다.\n",
    "\n",
    "- Dense Vector: BGE-M3의 다국어, 다중 작업 능력을 기반으로 함\n",
    "- Lexical weight를 활용한 sparse embedding으로 정확한 단어 매칭을 수행\n",
    "- ColBERT의 multi-vector 접근법으로 문맥을 고려한 세밀한 매칭 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for zlib-state (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [28 lines of output]\n",
      "      C:\\Users\\younghl\\AppData\\Local\\Temp\\pip-build-env-hhtwm2eo\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: MIT License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-313\\zlib_state\n",
      "      copying zlib_state\\__init__.py -> build\\lib.win-amd64-cpython-313\\zlib_state\n",
      "      running egg_info\n",
      "      writing zlib_state.egg-info\\PKG-INFO\n",
      "      writing dependency_links to zlib_state.egg-info\\dependency_links.txt\n",
      "      writing top-level names to zlib_state.egg-info\\top_level.txt\n",
      "      reading manifest file 'zlib_state.egg-info\\SOURCES.txt'\n",
      "      adding license file 'LICENSE'\n",
      "      writing manifest file 'zlib_state.egg-info\\SOURCES.txt'\n",
      "      running build_ext\n",
      "      building '_zlib_state' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for zlib-state\n",
      "error: failed-wheel-build-for-install\n",
      "\n",
      "× Failed to build installable wheels for some pyproject.toml based projects\n",
      "╰─> zlib-state\n"
     ]
    }
   ],
   "source": [
    "# FlagEmbedding 설치\n",
    "!pip install -qU FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44849f52a26b42fba011886641c7da40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "bge_embeddings = BGEM3FlagModel(\n",
    "    model_name, use_fp16=True\n",
    ")  # use_fp16을 True로 설정하면 약간의 성능 저하와 함께 계산 속도가 빨라집니다.\n",
    "\n",
    "bge_embedded = bge_embeddings.encode(\n",
    "    texts,\n",
    "    batch_size=12,\n",
    "    max_length=8192,  # 이렇게 긴 길이가 필요하지 않은 경우 더 작은 값을 설정하여 인코딩 프로세스의 속도를 높일 수 있습니다.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1024)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bge_embedded[\"dense_vecs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \t\tBAAI/bge-m3\n",
      "Dimension: \t1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: \\t\\t{model_name}\")\n",
    "print(f\"Dimension: \\t{len(embedded_documents[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96542211bdaa46dd903d0121f8bbdd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\", use_fp16=True\n",
    ")  # use_fp16을 True로 설정하면 약간의 성능 저하와 함께 계산 속도가 빨라집니다.\n",
    "bge_encoded = bge_flagmodel.encode(texts, return_dense=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1024)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 출력(행, 열)\n",
    "bge_encoded[\"dense_vecs\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Embedding (Lexical Weight)\n",
    "\n",
    "Sparse embedding은 벡터의 대부분의 값이 0인 고차원 벡터를 사용하는 임베딩 방식입니다. Lexical weight를 활용한 방식은 단어의 중요도를 고려하여 임베딩을 생성합니다.\n",
    "\n",
    "**작동 방식**\n",
    "1. 각 단어에 대해 lexical weight(어휘적 가중치)를 계산합니다. 이는 TF-IDF나 BM25 같은 방법을 사용할 수 있습니다.\n",
    "2. 문서나 쿼리의 각 단어에 대해, 해당 단어의 lexical weight를 사용하여 sparse vector의 해당 차원에 값을 할당합니다.\n",
    "3. 결과적으로 문서나 쿼리는 대부분의 값이 0인 고차원 벡터로 표현됩니다.\n",
    "\n",
    "**장점**\n",
    "- 단어의 중요도를 직접적으로 반영할 수 있습니다.\n",
    "- 특정 단어나 구문을 정확히 매칭할 수 있습니다.\n",
    "- 계산이 상대적으로 빠릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb3a788549b4348adbfaf8c5ae677c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\", use_fp16=True\n",
    ")  # use_fp16을 True로 설정하면 약간의 성능 저하와 함께 계산 속도가 빨라집니다.\n",
    "bge_encoded = bge_flagmodel.encode(texts, return_sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_vecs': array([[-0.00492401,  0.02456352, -0.052077  , ...,  0.01638757,\n",
       "         -0.02810185,  0.01218552],\n",
       "        [-0.0191887 , -0.0181497 , -0.00382788, ...,  0.02405743,\n",
       "          0.0128329 , -0.00620422],\n",
       "        [ 0.0146354 , -0.02158708,  0.00848752, ..., -0.01035915,\n",
       "          0.02252624,  0.01773892],\n",
       "        [-0.04505184, -0.02406329, -0.01311002, ...,  0.02105702,\n",
       "          0.02418931, -0.01861473],\n",
       "        [-0.01293309, -0.00730584, -0.01213078, ..., -0.00564261,\n",
       "          0.03795185,  0.00601621]], dtype=float32),\n",
       " 'lexical_weights': [defaultdict(int,\n",
       "              {'6888': 0.1594789,\n",
       "               '195269': 0.28882447,\n",
       "               '4': 0.1015741,\n",
       "               '54807': 0.25699177,\n",
       "               '2293': 0.14345205,\n",
       "               '20451': 0.12553936,\n",
       "               '713': 0.18402502,\n",
       "               '11529': 0.17917302,\n",
       "               '5': 0.11848412}),\n",
       "  defaultdict(int,\n",
       "              {'15786': 0.28657445,\n",
       "               '62467': 0.29970926,\n",
       "               '73': 0.19681458,\n",
       "               '112892': 0.21427521,\n",
       "               '1029': 0.17623803,\n",
       "               '90': 0.1531907,\n",
       "               '70': 0.017398274,\n",
       "               '9433': 0.110423185,\n",
       "               '111': 0.007249702,\n",
       "               '33976': 0.17502296,\n",
       "               '86685': 0.21807386,\n",
       "               '678': 0.07308903,\n",
       "               '21334': 0.10745708,\n",
       "               '46876': 0.19464724,\n",
       "               '115774': 0.2405866}),\n",
       "  defaultdict(int,\n",
       "              {'161545': 0.21388833,\n",
       "               '153816': 0.19317698,\n",
       "               '193751': 0.24564135,\n",
       "               '135903': 0.15396863,\n",
       "               '13916': 0.13787206,\n",
       "               '191268': 0.15485144,\n",
       "               '697': 0.035706207,\n",
       "               '15786': 0.19529435,\n",
       "               '62467': 0.22594243,\n",
       "               '73': 0.12228692,\n",
       "               '91872': 0.13209741,\n",
       "               '87750': 0.13109282,\n",
       "               '110309': 0.13545914,\n",
       "               '14137': 0.14642535,\n",
       "               '21417': 0.053115577,\n",
       "               '16907': 0.0813721,\n",
       "               '5358': 0.10068278,\n",
       "               '16385': 0.128901,\n",
       "               '126287': 0.05279956,\n",
       "               '175019': 0.08204334,\n",
       "               '77812': 0.12911284,\n",
       "               '713': 0.0013162643,\n",
       "               '6116': 0.061862066,\n",
       "               '124772': 0.048322283,\n",
       "               '210549': 0.07399637,\n",
       "               '48284': 0.16296631,\n",
       "               '1020': 0.019456469,\n",
       "               '44880': 0.028631702,\n",
       "               '42583': 0.096214995,\n",
       "               '21988': 0.0052730404,\n",
       "               '3292': 0.027254954}),\n",
       "  defaultdict(int,\n",
       "              {'15786': 0.26410127,\n",
       "               '62467': 0.28106394,\n",
       "               '73': 0.2121422,\n",
       "               '697': 0.11684285,\n",
       "               '22871': 0.14955454,\n",
       "               '9452': 0.113199495,\n",
       "               '3032': 0.096310794,\n",
       "               '96672': 0.18364304,\n",
       "               '166670': 0.23525436,\n",
       "               '1083': 0.0244748,\n",
       "               '240366': 0.2102181,\n",
       "               '127294': 0.17092392,\n",
       "               '2123': 0.02042561,\n",
       "               '192846': 0.10722437,\n",
       "               '124363': 0.18566191,\n",
       "               '3665': 0.17090234,\n",
       "               '8642': 0.13364324,\n",
       "               '5': 0.0046778433}),\n",
       "  defaultdict(int,\n",
       "              {'853': 0.14779416,\n",
       "               '97351': 0.22715108,\n",
       "               '1405': 0.18307847,\n",
       "               '9': 0.08702195,\n",
       "               '186432': 0.19002935,\n",
       "               '1183': 0.1607464,\n",
       "               '71': 0.11307594,\n",
       "               '83479': 0.26841265,\n",
       "               '15': 0.078797236,\n",
       "               '12280': 0.28190577,\n",
       "               '724': 0.31334507,\n",
       "               '16': 0.13801393,\n",
       "               '83': 0.1792903,\n",
       "               '142': 0.12626798,\n",
       "               '60266': 0.21777003,\n",
       "               '61353': 0.2412435,\n",
       "               '100': 0.1106807,\n",
       "               '224588': 0.23305616,\n",
       "               '38730': 0.27093366,\n",
       "               '57553': 0.24821971,\n",
       "               '7': 0.09845642,\n",
       "               '5': 0.033812836})],\n",
       " 'colbert_vecs': None}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bge_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3015604503452778\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "lexical_scores1 = bge_flagmodel.compute_lexical_matching_score(\n",
    "    bge_encoded[\"lexical_weights\"][0], bge_encoded[\"lexical_weights\"][0]\n",
    ")\n",
    "lexical_scores2 = bge_flagmodel.compute_lexical_matching_score(\n",
    "    bge_encoded[\"lexical_weights\"][0], bge_encoded[\"lexical_weights\"][1]\n",
    ")\n",
    "# 0 <-> 0\n",
    "print(lexical_scores1)\n",
    "# 0 <-> 1\n",
    "print(lexical_scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Vector (ColBERT)\n",
    "\n",
    "ColBERT(Contextualized Late Interaction over BERT)는 문서 검색을 위한 효율적인 방법입니다. 이 방식은 문서와 쿼리를 여러 개의 벡터로 표현하는 multi-vector 접근법을 사용합니다.\n",
    "\n",
    "**작동 방식**\n",
    "\n",
    "1. 문서의 각 토큰에 대해 별도의 벡터를 생성합니다. 즉, 하나의 문서는 여러 개의 벡터로 표현됩니다.\n",
    "2. 쿼리도 마찬가지로 각 토큰에 대해 별도의 벡터를 생성합니다.\n",
    "3. 검색 시, 쿼리의 각 토큰 벡터와 문서의 모든 토큰 벡터 사이의 유사도를 계산합니다.\n",
    "4. 이 유사도들을 종합하여 최종 검색 점수를 계산합니다.\n",
    "\n",
    "**장점**\n",
    "- 토큰 수준의 세밀한 매칭이 가능합니다.\n",
    "- 문맥을 고려한 임베딩을 생성할 수 있습니다.\n",
    "- 긴 문서에 대해서도 효과적으로 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa5ab99f561452bbd4f1d61a27fce6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "bge_flagmodel = BGEM3FlagModel(\n",
    "    \"BAAI/bge-m3\", use_fp16=True\n",
    ")  # use_fp16을 True로 설정하면 약간의 성능 저하와 함께 계산 속도가 빨라집니다.\n",
    "bge_encoded = bge_flagmodel.encode(texts, return_colbert_vecs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.3748)\n"
     ]
    }
   ],
   "source": [
    "colbert_scores1 = bge_flagmodel.colbert_score(\n",
    "    bge_encoded[\"colbert_vecs\"][0], bge_encoded[\"colbert_vecs\"][0]\n",
    ")\n",
    "colbert_scores2 = bge_flagmodel.colbert_score(\n",
    "    bge_encoded[\"colbert_vecs\"][0], bge_encoded[\"colbert_vecs\"][1]\n",
    ")\n",
    "# 0 <-> 0\n",
    "print(colbert_scores1)\n",
    "# 0 <-> 1\n",
    "print(colbert_scores2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
